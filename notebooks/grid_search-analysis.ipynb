{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important\n",
    "\n",
    "`make data/processed/ratings-train.csv` has to be run before running any notebook cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for optimal parameters values was conducted using GridSearch method. The whole process base only on training dataset, to avoid introducing a bias into test procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper methods to prepare data and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dicts(df):\n",
    "    if 'param_sim_options' in df.columns:\n",
    "        df_sim = df['param_sim_options'].apply(lambda x : dict(eval(x))).apply(pd.Series)\n",
    "        df = pd.concat([df, df_sim], axis=1).drop('param_sim_options', axis=1)\n",
    "    if 'param_bsl_options' in df.columns:\n",
    "        df_sim = df['param_bsl_options'].apply(lambda x : dict(eval(x))).apply(pd.Series)\n",
    "        df = pd.concat([df, df_sim], axis=1).drop('param_bsl_options', axis=1)\n",
    "    return df\n",
    "\n",
    "def select_cols(df, cols):\n",
    "    df = flatten_dicts(df)\n",
    "    cols = ['mean_test_rmse'] + cols + ['mean_fit_time', 'mean_test_time']\n",
    "    return df[cols]\n",
    "\n",
    "def compare(df, col):\n",
    "    return df.groupby(col)[['mean_test_rmse', 'mean_fit_time', 'mean_test_time']\n",
    "                       ].median().sort_values('mean_test_rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing similarity metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available metrics are:\n",
    "- `Cosine`:\n",
    "- `Mean Squared Difference`: \n",
    "- `Pearson`: \n",
    "- `Pearson with baseline`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because number of users is much higher than number of items, we use item-based similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shrinkage parameter can be specified for Pearson Baseline to avoid overfitting when only few ratings are available. In our dataset there are always at least 8 ratings for books, so there is no need to tune this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_metric = pd.read_csv(\"../results/knn-parameters-search-sim_metric.csv\", index_col='rank_test_rmse')\n",
    "df_sim_metric = select_cols(df_sim_metric, ['name'])\n",
    "compare(df_sim_metric, 'name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson baseline methods achieves the best results due to the fact, that it take into account the baselines. Further explained in section 2.1 of \"Factor in the Neighbors: Scalable and\n",
    "Accurate Collaborative Filtering\" by Koren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing baselines estimates method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available methods are:\n",
    "- `SGD`: Stochastic Gradient Descent\n",
    "- `ALS`: Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baselines = pd.read_csv(\"../results/knn-parameters-search-baselines.csv\", index_col='rank_test_rmse')\n",
    "df_baselines = select_cols(df_baselines, ['method', 'n_epochs', 'reg', 'learning_rate', 'reg_i', 'reg_u'])\n",
    "compare(df_baselines, 'method')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALS and SGD achieve comparable results with deafult parameters, but ALS is trained faster. Therefore, we choose ALS for tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing neighbors count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available parameters are:\n",
    "- `k`: maximal number of neighbors to take into account; default value 40\n",
    "- `min_support`: minimal number of similar users between neighbor and current item for calculating similarity isntead of returning 0; default value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbors = pd.read_csv(\"../results/knn-parameters-search-neighbors.csv\", index_col='rank_test_rmse')\n",
    "df_neighbors = select_cols(df_neighbors, ['param_k', 'min_support'])\n",
    "df_neighbors.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(df_neighbors, ['param_k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into consideration smaller number of neighbors seems to benefit the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(df_neighbors, ['min_support'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, any value for score is better than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing regularization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_reg = pd.read_csv(\"../results/knn-parameters-search-reg.csv\", index_col='rank_test_rmse')\n",
    "df_knn_reg = select_cols(df_knn_reg, ['n_epochs', 'reg_i', 'reg_u'])\n",
    "df_knn_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(df_knn_reg, ['n_epochs', 'reg_i', 'reg_u'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no huge differences between obtained results, so we stick to the defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {'bsl_options': {'method': ['als'],\n",
    "                          'reg_i': [10],\n",
    "                          'reg_u': [15],\n",
    "                          'n_epochs': [10]},\n",
    "          'k': [30],\n",
    "          'sim_options': {'name': ['pearson_baseline'],\n",
    "                          'min_support': [1],\n",
    "                          'user_based': [False],\n",
    "                          'shrinkage': [100]},\n",
    "          'verbose': [False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We opted for SVD instead of SVD++. The latter requires far more time for training phase(5 vs 150 minutes) and scores better by only 0.01 points(0.82 vs 0.81)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing factors number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_factors = pd.read_csv(\"../results/svd-parameters-search-factors.csv\", index_col='rank_test_rmse')\n",
    "df_factors = select_cols(df_factors, ['param_n_factors'])\n",
    "compare(df_factors, 'param_n_factors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, higher number of factors does not result in better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing regularization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = pd.read_csv(\"../results/svd-parameters-search-init.csv\", index_col='rank_test_rmse')\n",
    "df_init = select_cols(df_init, ['param_init_mean', 'param_init_std_dev'])\n",
    "df_init.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the ratings mean is much greater than average point of the scale(3.9 vs 2.5) we can assume than starting from median closer to real one would yield better results. That turned out to be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svd_reg = pd.read_csv(\"../results/svd-parameters-search-reg.csv\", index_col='rank_test_rmse')\n",
    "df_svd_reg = select_cols(df_svd_reg, ['param_n_epochs', 'param_lr_all', 'param_reg_all'])\n",
    "df_svd_reg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default parameters for surprise library were already adjusted for 1-5 rating scale. Therefore different parameter values give worse results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_params = {'n_factors': [100],\n",
    "          'biased': [True],\n",
    "          'init_mean': [0.1],\n",
    "          'init_std_dev': [0.05],\n",
    "          'n_epochs': [25],\n",
    "          'lr_all': [0.005],\n",
    "          'reg_all': [0.02],\n",
    "          'random_state': [44]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs-venv",
   "language": "python",
   "name": "rs-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
