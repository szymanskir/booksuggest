{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important\n",
    "\n",
    "`make scores` has to be run before any cell in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('../results/cb-results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to describe and evaluate recommendation models validation methods that were used during the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method was to using the classic precision and recall measures that are often used in classification problems. The idea consisted of comparing the set of books recommended by models and the set of books that were found in the `similar_books` tag in the xml data files. This method was used only for the purpose of comparing different models that use different representations of the items of interest and deciding which one is the best performing. The following results were obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['model', 'precision', 'recall']].sort_values(\n",
    "    ['precision', 'recall'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf-idf models performed better than count models, tags features also usually resulted in better accuracy. However, for all models scores are rather low, but does it mean that all models are not working properly?\n",
    "\n",
    "The recommendation problem is much more complex than the classification problem. The main difference is the subjective side of recommendations e.g. one book may be a good recommendation for one person but a bad one for another person. In case of classifications labels can usually be described objectively e.g. the picture presents the 1, 2, 3 digits.\n",
    "\n",
    "Another issue is that if a book recommended by the implemented model was not in the test set, it does not necessarily mean that it is not a good recommendation.\n",
    "\n",
    "While collecting ground truth data on similar books several phenomenons can occur. Let's consider the following example: the goal is to collect data about books that are similar to A, there are two books B and C that are similar to A. However B is more popular and C is more similar to A. The problem is that B will appear more frequently in the test data just because more people have read this book and will consequently be considered as the more similar one even though C is the more similar one.\n",
    "\n",
    "The whole definition of `similar books` is very ambiguous. One might consider books to be similar because the main characters behave in a similar way, but the stories have a significantly different setting. The style of writing is also a factor that also determines whether books are similar.\n",
    "\n",
    "Due to the reasons described above the precision and recall metrics do not represent the overall performance of models, but it can be a way of comparing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified precision and recall measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the previous section the precision and recall measures penalize the model when it's recommendations are not the present in the test set. The idea was to remove that property and only consider the positive feedback.\n",
    "\n",
    "As all recommendation models recommend the same amount of books, the idea was to compare the amount of recommendations that were also present in the test set. The goal was to find a measurable difference between the models in order to define some characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['model', 'correct_hits']].sort_values(\n",
    "    'correct_hits', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The tags features have majorely enhanced the precision and recall scores of the models. However due to the complexity of the recommendation problem those measures are not sufficient to determine the overall performance of models. Further online evaluation methods are needed in order to confirm if the determined business goals are being achieved and subsequent models should be designed in order to maximize the online quality measure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
